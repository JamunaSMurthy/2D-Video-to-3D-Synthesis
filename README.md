# **2D Video to 3D Scene Synthesis**
ğŸ”¹ Transforming 2D videos into immersive 3D scenes using deep learning and depth estimation.

<p align="center"> 
  <img src="assets/sample.gif" width="700px">
</p>

---

## ğŸ“ Overview
This project aims to convert **monocular 2D video** into **3D scene representations** using **depth estimation, point cloud generation, and neural rendering techniques**. It reconstructs depth information from a single-view video and renders a **realistic 3D environment** with smooth camera movements.

---

## âœ¨ Key Features
- âœ… **Monocular Depth Estimation** (MiDaS-based)  
- âœ… **Point Cloud Generation** (Open3D)  
- âœ… **Neural Network-Based Rendering**  
- âœ… **Depth-Aware Occlusion Handling**  
- âœ… **Real-Time 3D Scene Visualization**  

---

## ğŸ” Motivation  
The demand for **3D content** is increasing in **AR/VR, gaming, and autonomous systems**. However, traditional **multi-camera setups** for 3D scene reconstruction are **expensive and impractical**. This project introduces an **efficient pipeline** that enables **2D-to-3D transformation using a single video source**.

### ğŸ”¹ Applications  
- âœ”ï¸ **Virtual Reality & Augmented Reality (VR/AR)**  
- âœ”ï¸ **Autonomous Navigation (Robotics, Self-Driving Cars)**  
- âœ”ï¸ **Digital Content Creation & Game Development**  
- âœ”ï¸ **Medical Imaging & 3D Reconstruction from Endoscopy Videos**  

---

## ğŸ› ï¸ Methodology
This project follows an **end-to-end pipeline** to reconstruct **3D scenes** from **2D video frames**.


### **1ï¸âƒ£ Depth Estimation**  
- Uses the **MiDaS depth estimation model** to predict **per-frame depth maps**.  
- The model processes each video frame and outputs a **depth map representing spatial structure**.

### ğŸ“Œ **Depth Estimation Architecture**
<p align="center"> 
  <img src="assets/Architecture.jpg" width="700px">
</p>

### **2ï¸âƒ£ Point Cloud Generation**  
- Converts **depth maps** into **3D point clouds** using **intrinsic camera parameters**.  
- **Open3D** is used for **processing and rendering the point cloud**.  

### ğŸ“Œ **Point Cloud Generation Architecture**
<p align="center"> 
  <img src="assets/point_architecture.png" width="700px">
</p>

### **3ï¸âƒ£ 3D Scene Rendering**  
- **Neural rendering** is applied to generate **high-quality 3D images**.  
- **Depth-aware occlusion handling** ensures **realistic object overlap and perspective**.  

### ğŸ“Œ **3D Scene Rendering Architecture**
<p align="center"> 
  <img src="assets/neuralrendering.jpg" width="700px">
</p>

### **4ï¸âƒ£ Camera Movement Simulation**  
- Generates **smooth 3D video** using a **dynamic camera trajectory** around the reconstructed scene.  
- **Gaussian smoothing** is applied to improve rendering quality.

### ğŸ“Œ **Camera Movement Simulation Architecture**
<p align="center"> 
  <img src="assets/camera.png" width="700px">
</p>

---

## ğŸ“Š Results & Evaluation  
The system successfully converts **2D videos** into **realistic 3D visualizations**.  
ğŸ“Œ **Visual Quality:** High-fidelity 3D reconstructions with smooth depth transitions.  

### ğŸ“Œ **Sample Outputs:**

### **1ï¸âƒ£ Depth Map Generation**  
<p align="center">
  <img src="assets/depth.png" width="500px">
</p>

### **2ï¸âƒ£ Point Cloud Visualization**  
<p align="center">
  <img src="assets/pointcloud.png" width="500px">
</p>

### **3ï¸âƒ£ Final 3D Scene Rendering**  
<p align="center">
  <img src="assets/rendering.png" width="500px">
</p>

âœ… **Smooth camera motion around 3D objects**  
âœ… **Realistic depth perception and scene occlusion**  
âœ… **Scalable for various video sources and camera angles**  

---

## ğŸ“¥ Installation & Usage  

### ğŸ”§ **Setup Instructions**
```bash
# Clone the repository
git clone https://github.com/JamunaSMurthy/2D-Video-to-3D-Synthesis.git
cd 2D-Video-to-3D-Synthesis

# Install dependencies
pip install -r requirements.txt
```
---
## â–¶ï¸ Running the Pipeline
```bash
python main.py --input video.mp4 --output output_3d.mp4
```
Arguments:

- input : Path to input 2D video.
- output : Path to save the generated 3D video.
âš™ï¸ Implementation Details
---
ğŸ“Œ Frameworks & Tools Used:

- PyTorch - Neural network processing
- MiDaS - Depth estimation model
- Open3D - Point cloud visualization
- OpenCV - Video processing
- NumPy - Numerical computations

ğŸ“Œ Hardware Requirements:

- âœ… NVIDIA GPU (Recommended) for real-time processing
- âœ… Compatible with Google Colab for cloud execution

---

## ğŸ”¬ Future Enhancements

ğŸš€ Planned Improvements:

- âœ”ï¸ Support for Real-Time Processing
- âœ”ï¸ Multi-View Synthesis for Enhanced 3D Realism
- âœ”ï¸ Integration with Neural Radiance Fields (NeRF)
---
ğŸ“œ Citation & References

If you use this work, please cite this repository:

```bash
@misc{JamunaSMurthy2025,
  author = {Jamuna S Murthy},
  title = {2D Video to 3D Scene Synthesis Using Depth Estimation and Rendering},
  year = {2025},
  howpublished = {\url{https://github.com/JamunaSMurthy/2D-Video-to-3D-Synthesis/}},
  note = {GitHub repository}
}
```

ğŸ“– Related Works:

### 1ï¸âƒ£MiDaS: Vision Transformers for Depth Estimation â€“ GitHub (https://github.com/isl-org/MiDaS)
### 2ï¸âƒ£ Open3D: 3D Data Processing Library â€“ Website(https://www.open3d.org)
### 3ï¸âƒ£ Neural Radiance Fields (NeRF) for View Synthesis â€“ Paper(https://arxiv.org/abs/2003.08934)

ğŸ‘¨â€ğŸ’» Contributors

@JamunaSMurthy
ğŸš€ Feel free to contribute to this repository by submitting issues or pull requests!

â­ Support & Feedback

If you find this project useful, please â­ star this repository and share your feedback!





